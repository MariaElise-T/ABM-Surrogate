{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3f4010-5b53-411f-9512-5b9302ef44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.optim import Adam\n",
    "#import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe.iloc[:, :3].values, dtype=torch.float32)  # Input: [Batch, 3]\n",
    "        self.Y = torch.tensor(dataframe.iloc[:, 3:].values, dtype=torch.float32).unsqueeze(-1) # Output: [Batch, 255, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    " \n",
    "        # Input Encoder (maps input to d_model size)\n",
    "        self.encoder = nn.Linear(input_dim, d_model)  # (Batch, 3) -> (Batch, d_model)\n",
    "        \n",
    "        # Project input to match the sequence length\n",
    "        self.expand_input = nn.Linear(d_model, seq_length * d_model)  # (Batch, d_model) -> (Batch, seq_length * d_model)\n",
    "        \n",
    "        # Target embedding for decoder input\n",
    "        self.target_embedding = nn.Linear(1, d_model)  # New embedding layer for target sequence\n",
    "  \n",
    "        # Positional Encoding for Time Steps\n",
    "        self.pos_encoder = PositionalEncoding(d_model, seq_length)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)  # (Batch, 255, d_model) -> (Batch, 255, 1)\n",
    "\n",
    "    def forward(self, x, target_seq):\n",
    "        # x: Input features [Batch, 3]\n",
    "        # target_seq: Target sequence for teacher forcing [Batch, 255, 1]\n",
    "        \n",
    "        # Encode input features\n",
    "        encoded_input = self.encoder(x)  # [Batch, d_model]\n",
    "        \n",
    "        # Expand input to match sequence length\n",
    "        expanded_input = self.expand_input(encoded_input)  # [Batch, seq_length * d_model]\n",
    "        expanded_input = expanded_input.view(-1, self.seq_length, self.d_model)  # Reshape to [Batch, 255, d_model]\n",
    "        \n",
    "        # Add Positional Encoding\n",
    "        expanded_input = self.pos_encoder(expanded_input)\n",
    "        \n",
    "        # Process the target sequence through the same encoding pipeline\n",
    "  #      target_embeddings = self.encoder(target_seq)\n",
    "  #      target_embeddings = nn.Linear(1, d_model)(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.target_embedding(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.pos_encoder(target_embeddings)\n",
    "        \n",
    "        # Decode sequence\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=target_embeddings, memory=expanded_input\n",
    "        )  # Output shape: [Batch, 255, d_model]\n",
    "        \n",
    "        # Map to output dimensions\n",
    "        predictions = self.output_layer(output)  # [Batch, 255, 1]\n",
    "        return predictions\n",
    "    \n",
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs, device):\n",
    "    loss_list = list()\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            x, y = batch  # x: [Batch, N], y: [Batch, T]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Prepare target for teacher forcing\n",
    "            target_seq = y \n",
    "            #target_seq = y[:, :-1]  # All except last time step\n",
    "            #actual = y[:, 1:]       # All except first time step\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(x, target_seq)\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    return loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9cb86f-c405-42ee-842d-ca2bb172ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "\n",
      "Max value is : 1451 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Set up      \n",
    "##################################################################\n",
    "\n",
    "print(\"Importing data...\\n\")\n",
    "\n",
    "data_output = pd.read_csv(\"~/Desktop/TS-Clustering/SimData/epsteinCV_outputs_active.csv\", header=None)\n",
    "print(\"Max value is :\", data_output.to_numpy().max(), \"\\n\")\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, data_output.to_numpy().max()))\n",
    "scaler.fit(data_output)\n",
    "data_output = scaler.transform(data_output)\n",
    "data_output = pd.DataFrame(data_output)\n",
    "\n",
    "data_input = pd.read_csv(\"~/Desktop/TS-Clustering/SimData/epsteinCV_inputs.csv\", sep=\" \", header=None)\n",
    "data = pd.concat([data_input, data_output], axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the validation set to a new CSV file\n",
    "valid_data.to_csv(\"validation_set_epstein.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41645c96-b643-4c8d-9b57-1a1ddae70a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model 1\n",
      "\n",
      "Instatiate \n",
      "\n",
      "Train\n",
      "\n",
      "Epoch 1/20, Loss: 331657.125\n",
      "Epoch 2/20, Loss: 553.4930419921875\n",
      "Epoch 3/20, Loss: 23349.3515625\n",
      "Epoch 4/20, Loss: 528554.1875\n",
      "Epoch 5/20, Loss: 36697.60546875\n",
      "Epoch 6/20, Loss: 17380.27734375\n",
      "Epoch 7/20, Loss: 20159.275390625\n",
      "Epoch 8/20, Loss: 223107.234375\n",
      "Epoch 9/20, Loss: 370103.96875\n",
      "Epoch 10/20, Loss: 883563.0\n",
      "Epoch 11/20, Loss: 483048.4375\n",
      "Epoch 12/20, Loss: 7868.3876953125\n",
      "Epoch 13/20, Loss: 4206.35595703125\n",
      "Epoch 14/20, Loss: 2425.756103515625\n",
      "Epoch 15/20, Loss: 25472.064453125\n",
      "Epoch 16/20, Loss: 1116486.25\n",
      "Epoch 17/20, Loss: 11565.4912109375\n",
      "Epoch 18/20, Loss: 849.1578369140625\n",
      "Epoch 19/20, Loss: 14909.6806640625\n",
      "Epoch 20/20, Loss: 29997.72265625\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Model 1      \n",
    "##################################################################\n",
    "\n",
    "print(\"Starting model 1\\n\")\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "print(\"Instatiate \\n\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "print(\"Train\\n\")\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust based on dataset size and performance\n",
    "loss = train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)\n",
    "\n",
    "df = pd.DataFrame(loss, columns=[\"loss\"])\n",
    "df.to_csv('transformer_adam_lr001_epstein_loss.csv', index=False)\n",
    "torch.save(model.state_dict(), \"transformer_adam_lr001_epstein.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832c5498-5003-4a3e-a059-7bd831d1d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model 2\n",
      "\n",
      "Epoch 1/20, Loss: 24429.005859375\n",
      "Epoch 2/20, Loss: 19443.615234375\n",
      "Epoch 3/20, Loss: 16719.541015625\n",
      "Epoch 4/20, Loss: 732787.0625\n",
      "Epoch 5/20, Loss: 18382.765625\n",
      "Epoch 6/20, Loss: 20425.94921875\n",
      "Epoch 7/20, Loss: 17974.16796875\n",
      "Epoch 8/20, Loss: 24082.98046875\n",
      "Epoch 9/20, Loss: 18664.146484375\n",
      "Epoch 10/20, Loss: 19070.171875\n",
      "Epoch 11/20, Loss: 16628.61328125\n",
      "Epoch 12/20, Loss: 18840.94921875\n",
      "Epoch 13/20, Loss: 18188.232421875\n",
      "Epoch 14/20, Loss: 21944.369140625\n",
      "Epoch 15/20, Loss: 21960.646484375\n",
      "Epoch 16/20, Loss: 20113.146484375\n",
      "Epoch 17/20, Loss: 19022.21484375\n",
      "Epoch 18/20, Loss: 23376.65625\n",
      "Epoch 19/20, Loss: 12144.6728515625\n",
      "Epoch 20/20, Loss: 14653.7939453125\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Model 2      \n",
    "##################################################################\n",
    "\n",
    "print(\"Starting model 2\\n\")\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust based on dataset size and performance\n",
    "loss = train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)\n",
    "\n",
    "df = pd.DataFrame(loss, columns=[\"loss\"])\n",
    "df.to_csv('transformer_adam_lr01_epstein_loss.csv', index=False)\n",
    "torch.save(model.state_dict(), \"transformer_adam_lr01_epstein.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aca5d9-5ff4-47b7-9204-e3375cc73ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Model 3      \n",
    "##################################################################\n",
    "\n",
    "print(\"Starting model 3\\n\")\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust based on dataset size and performance\n",
    "loss = train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)\n",
    "\n",
    "df = pd.DataFrame(loss, columns=[\"loss\"])\n",
    "df.to_csv('transformer_adam_lr0001_epstein_loss.csv', index=False)\n",
    "torch.save(model.state_dict(), \"transformer_adam_lr0001_epstein.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222f00b-7730-4850-8718-9ead7827579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Model 4      \n",
    "##################################################################\n",
    "\n",
    "print(\"Starting model 4\\n\")\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust based on dataset size and performance\n",
    "loss = train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)\n",
    "\n",
    "df = pd.DataFrame(loss, columns=[\"loss\"])\n",
    "df.to_csv('transformer_adamW_lr001_epstein_loss.csv', index=False)\n",
    "torch.save(model.state_dict(), \"transformer_adamW_lr001_epstein.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c54503-85aa-4152-a44e-b62bde488c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Model 5      \n",
    "##################################################################\n",
    "\n",
    "print(\"Starting model 5\\n\")\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust based on dataset size and performance\n",
    "loss = train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)\n",
    "\n",
    "df = pd.DataFrame(loss, columns=[\"loss\"])\n",
    "df.to_csv('transformer_adamW_lr01_epstein_loss.csv', index=False)\n",
    "torch.save(model.state_dict(), \"transformer_adamW_lr01_epstein.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25991466-fa9d-4273-a65a-0782b50df743",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Model 6      \n",
    "##################################################################\n",
    "\n",
    "print(\"Starting model 6\\n\")\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Adjust based on dataset size and performance\n",
    "loss = train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)\n",
    "\n",
    "df = pd.DataFrame(loss, columns=[\"loss\"])\n",
    "df.to_csv('transformer_adamW_lr0001_epstein_loss.csv', index=False)\n",
    "torch.save(model.state_dict(), \"transformer_adamW_lr0001_epstein.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
