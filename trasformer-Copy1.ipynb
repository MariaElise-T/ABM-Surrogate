{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "siboVxRJ5pes"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe.iloc[:, :1].values, dtype=torch.float32)  # Input: [Batch, 3]\n",
    "        self.Y = torch.tensor(dataframe.iloc[:, 1:].values, dtype=torch.float32).unsqueeze(-1) # Output: [Batch, 255, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Time Steps\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    " \n",
    "        # Input Encoder (maps input to d_model size)\n",
    "        self.encoder = nn.Linear(input_dim, d_model)  # (Batch, 3) -> (Batch, d_model)\n",
    "        \n",
    "        # Project input to match the sequence length\n",
    "        self.expand_input = nn.Linear(d_model, seq_length * d_model)  # (Batch, d_model) -> (Batch, seq_length * d_model)\n",
    "        \n",
    "        # Target embedding for decoder input\n",
    "        self.target_embedding = nn.Linear(1, d_model)  # New embedding layer for target sequence\n",
    "  \n",
    "        # Positional Encoding for Time Steps\n",
    "        self.pos_encoder = PositionalEncoding(d_model, seq_length)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)  # (Batch, 255, d_model) -> (Batch, 255, 1)\n",
    "\n",
    "    def forward(self, x, target_seq):\n",
    "        # x: Input features [Batch, 3]\n",
    "        # target_seq: Target sequence for teacher forcing [Batch, 255, 1]\n",
    "        \n",
    "        # Encode input features\n",
    "        encoded_input = self.encoder(x)  # [Batch, d_model]\n",
    "        \n",
    "        # Expand input to match sequence length\n",
    "        expanded_input = self.expand_input(encoded_input)  # [Batch, seq_length * d_model]\n",
    "        expanded_input = expanded_input.view(-1, self.seq_length, self.d_model)  # Reshape to [Batch, 255, d_model]\n",
    "        \n",
    "        # Add Positional Encoding\n",
    "        expanded_input = self.pos_encoder(expanded_input)\n",
    "        \n",
    "        # Process the target sequence through the same encoding pipeline\n",
    "  #      target_embeddings = self.encoder(target_seq)\n",
    "  #      target_embeddings = nn.Linear(1, d_model)(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.target_embedding(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.pos_encoder(target_embeddings)\n",
    "        \n",
    "        # Decode sequence\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=target_embeddings, memory=expanded_input\n",
    "        )  # Output shape: [Batch, 255, d_model]\n",
    "        \n",
    "        # Map to output dimensions\n",
    "        predictions = self.output_layer(output)  # [Batch, 255, 1]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            x, y = batch  # x: [Batch, N], y: [Batch, T]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Prepare target for teacher forcing\n",
    "            target_seq = y \n",
    "            #target_seq = y[:, :-1]  # All except last time step\n",
    "            #actual = y[:, 1:]       # All except first time step\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(x, target_seq)\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data_input = pd.read_csv(\"~/Desktop/TS-Clustering/SimData/bank_reserves_inputs.csv\", sep=\" \", header=None)\n",
    "data_output = pd.read_csv(\"~/Desktop/TS-Clustering/SimData/bank_reserves_outputs_poor.csv\", header=None)\n",
    "data = pd.concat([data_input, data_output], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the validation set to a new CSV file\n",
    "valid_data.to_csv(\"validation_set2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75220</th>\n",
       "      <td>0.848036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.109195</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.097701</td>\n",
       "      <td>0.091429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.092486</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48955</th>\n",
       "      <td>0.629694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398844</td>\n",
       "      <td>0.385057</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.403409</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>0.373563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44966</th>\n",
       "      <td>0.574623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.454023</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.485549</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.462857</td>\n",
       "      <td>0.448864</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.462428</td>\n",
       "      <td>0.459770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13568</th>\n",
       "      <td>0.244526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826590</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.867052</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.826590</td>\n",
       "      <td>0.804598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92727</th>\n",
       "      <td>0.769596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.205714</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.205714</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.179191</td>\n",
       "      <td>0.183908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3      4         5         6         7         8    \\\n",
       "75220  0.848036  0.0  0.0  0.0  0.125  0.083333  0.176471  0.318182  0.250000   \n",
       "48955  0.629694  0.0  0.0  0.0  0.000  0.083333  0.117647  0.318182  0.285714   \n",
       "44966  0.574623  0.0  0.0  0.0  0.000  0.083333  0.176471  0.363636  0.357143   \n",
       "13568  0.244526  0.0  0.0  0.0  0.125  0.083333  0.176471  0.363636  0.321429   \n",
       "92727  0.769596  0.0  0.0  0.0  0.250  0.166667  0.294118  0.318182  0.464286   \n",
       "\n",
       "            9    ...       92        93        94        95        96   \\\n",
       "75220  0.272727  ...  0.104046  0.109195  0.108571  0.104046  0.097701   \n",
       "48955  0.333333  ...  0.398844  0.385057  0.405714  0.393064  0.419540   \n",
       "44966  0.303030  ...  0.450867  0.454023  0.497143  0.485549  0.465517   \n",
       "13568  0.545455  ...  0.826590  0.839080  0.851429  0.867052  0.844828   \n",
       "92727  0.363636  ...  0.213873  0.212644  0.205714  0.202312  0.195402   \n",
       "\n",
       "            97        98        99        100       101  \n",
       "75220  0.091429  0.090909  0.093023  0.092486  0.091954  \n",
       "48955  0.405714  0.403409  0.406977  0.393064  0.373563  \n",
       "44966  0.462857  0.448864  0.453488  0.462428  0.459770  \n",
       "13568  0.828571  0.829545  0.837209  0.826590  0.804598  \n",
       "92727  0.205714  0.193182  0.186047  0.179191  0.183908  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame (assuming it's named `df`)\n",
    "dataset = TimeSeriesDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 1      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 101   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.00027698182384483516\n",
      "Epoch 2/40, Loss: 8.254119893535972e-05\n",
      "Epoch 3/40, Loss: 3.659398862509988e-05\n",
      "Epoch 4/40, Loss: 5.7432316680205986e-05\n",
      "Epoch 5/40, Loss: 8.650434028822929e-05\n",
      "Epoch 6/40, Loss: 1.3282042345963418e-05\n",
      "Epoch 7/40, Loss: 7.425429066643119e-05\n",
      "Epoch 8/40, Loss: 1.1242395885346923e-05\n",
      "Epoch 9/40, Loss: 3.737273073056713e-05\n",
      "Epoch 10/40, Loss: 5.978406079520937e-06\n",
      "Epoch 11/40, Loss: 5.410883204604033e-06\n",
      "Epoch 12/40, Loss: 2.713287904043682e-05\n",
      "Epoch 13/40, Loss: 3.175680376443779e-06\n",
      "Epoch 14/40, Loss: 4.0141203498933464e-06\n",
      "Epoch 15/40, Loss: 2.8936549369973363e-06\n",
      "Epoch 16/40, Loss: 2.6253683245158754e-06\n",
      "Epoch 17/40, Loss: 1.2971908063263982e-06\n",
      "Epoch 18/40, Loss: 5.5115565373853315e-06\n",
      "Epoch 19/40, Loss: 5.008102561987471e-06\n",
      "Epoch 20/40, Loss: 5.974634404992685e-07\n",
      "Epoch 21/40, Loss: 1.5452789739356376e-05\n",
      "Epoch 22/40, Loss: 2.8296651635173475e-06\n",
      "Epoch 23/40, Loss: 2.289004669364658e-06\n",
      "Epoch 24/40, Loss: 8.549888548259332e-07\n",
      "Epoch 25/40, Loss: 7.244902349157201e-07\n",
      "Epoch 26/40, Loss: 8.678591257194057e-05\n",
      "Epoch 27/40, Loss: 3.147361269384419e-07\n",
      "Epoch 28/40, Loss: 6.778849979127699e-07\n",
      "Epoch 29/40, Loss: 6.505379701593483e-07\n",
      "Epoch 30/40, Loss: 5.216176646172244e-07\n",
      "Epoch 31/40, Loss: 1.926492359416443e-06\n",
      "Epoch 32/40, Loss: 1.2739616295220912e-06\n",
      "Epoch 33/40, Loss: 4.2123400589844096e-07\n",
      "Epoch 34/40, Loss: 1.8683348343984107e-06\n",
      "Epoch 35/40, Loss: 8.41224846226396e-07\n",
      "Epoch 36/40, Loss: 1.4288680176832713e-06\n",
      "Epoch 37/40, Loss: 1.0363864930695854e-06\n",
      "Epoch 38/40, Loss: 1.5475060308745014e-06\n",
      "Epoch 39/40, Loss: 1.2829638080802397e-06\n",
      "Epoch 40/40, Loss: 2.959669131996634e-07\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40  # Adjust based on dataset size and performance\n",
    "train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_adam_lr001_bankreserves.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim=1, output_dim=1, seq_length=101, \n",
    "    d_model=128, nhead=4, num_layers=2, dim_feedforward=512\n",
    ")\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(\"transformer_adam_lr001_bankreserves.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"transformer_adam_lr001_epstein.pth\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "batch_size = 1  # Inference on a single sample\n",
    "input_features = torch.tensor([[0.5, 0.5, 0.5]], dtype=torch.float32)  # Shape: [1, 3]\n",
    "\n",
    "# Initialize target sequence with zeros for inference\n",
    "seq_length = 252\n",
    "target_seq = torch.zeros((batch_size, seq_length, 1), dtype=torch.float32)  # Shape: [1, 255, 1]\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_features, target_seq)  # Output shape: [1, 255, 1]\n",
    "\n",
    "# Convert to NumPy array for easy viewing\n",
    "predicted_values = predictions.squeeze().numpy()  # Shape: [255]\n",
    "print(predicted_values)  # Print first 10 predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeriesModel(\n",
       "  (encoder): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (expand_input): Linear(in_features=128, out_features=12928, bias=True)\n",
       "  (target_embedding): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bank_reserves_inputs_surr.csv', 'w', newline='') as bank_reserves_inputs, \\\n",
    "    open('bank_reserves_outputs_poor_surr.csv', 'w', newline='') as bank_reserves_outputs_poor:\n",
    "    for i in np.arange(100000):\n",
    "        reserve_perc = random.uniform(0,100)\n",
    "        bankRes = bankReservesModel.BankReserves(init_people=500, rich_threshold=10, reserve_percent=reserve_perc)\n",
    "        bankRes.run_model()\n",
    "        results = bankRes.datacollector.get_model_vars_dataframe()\n",
    "        print(reserve_perc, file=bank_reserves_inputs)\n",
    "        print(*results['Rich'].to_list(), file=bank_reserves_outputs_rich, sep=\",\")\n",
    "        print(*results['Middle Class'].to_list(), file=bank_reserves_outputs_middle, sep=\",\")\n",
    "        print(*results['Poor'].to_list(), file=bank_reserves_outputs_poor, sep=\",\")\n",
    "bank_reserves_inputs.close()\n",
    "bank_reserves_outputs_poor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
