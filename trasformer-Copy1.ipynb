{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "siboVxRJ5pes"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Visualization tools\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe.iloc[:, :1].values, dtype=torch.float32)  # Input: [Batch, 3]\n",
    "        self.Y = torch.tensor(dataframe.iloc[:, 1:].values, dtype=torch.float32).unsqueeze(-1) # Output: [Batch, 255, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Time Steps\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    " \n",
    "        # Input Encoder (maps input to d_model size)\n",
    "        self.encoder = nn.Linear(input_dim, d_model)  # (Batch, 3) -> (Batch, d_model)\n",
    "        \n",
    "        # Project input to match the sequence length\n",
    "        self.expand_input = nn.Linear(d_model, seq_length * d_model)  # (Batch, d_model) -> (Batch, seq_length * d_model)\n",
    "        \n",
    "        # Target embedding for decoder input\n",
    "        self.target_embedding = nn.Linear(1, d_model)  # New embedding layer for target sequence\n",
    "  \n",
    "        # Positional Encoding for Time Steps\n",
    "        self.pos_encoder = PositionalEncoding(d_model, seq_length)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)  # (Batch, 255, d_model) -> (Batch, 255, 1)\n",
    "\n",
    "    def forward(self, x, target_seq):\n",
    "        # x: Input features [Batch, 3]\n",
    "        # target_seq: Target sequence for teacher forcing [Batch, 255, 1]\n",
    "        \n",
    "        # Encode input features\n",
    "        encoded_input = self.encoder(x)  # [Batch, d_model]\n",
    "        \n",
    "        # Expand input to match sequence length\n",
    "        expanded_input = self.expand_input(encoded_input)  # [Batch, seq_length * d_model]\n",
    "        expanded_input = expanded_input.view(-1, self.seq_length, self.d_model)  # Reshape to [Batch, 255, d_model]\n",
    "        \n",
    "        # Add Positional Encoding\n",
    "        expanded_input = self.pos_encoder(expanded_input)\n",
    "        \n",
    "        # Process the target sequence through the same encoding pipeline\n",
    "  #      target_embeddings = self.encoder(target_seq)\n",
    "  #      target_embeddings = nn.Linear(1, d_model)(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.target_embedding(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.pos_encoder(target_embeddings)\n",
    "        \n",
    "        # Decode sequence\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=target_embeddings, memory=expanded_input\n",
    "        )  # Output shape: [Batch, 255, d_model]\n",
    "        \n",
    "        # Map to output dimensions\n",
    "        predictions = self.output_layer(output)  # [Batch, 255, 1]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            x, y = batch  # x: [Batch, N], y: [Batch, T]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Prepare target for teacher forcing\n",
    "            target_seq = y \n",
    "            #target_seq = y[:, :-1]  # All except last time step\n",
    "            #actual = y[:, 1:]       # All except first time step\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(x, target_seq)\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data_input = pd.read_csv(\"C:/Users/met48/Desktop/TS-Clustering/SimData/bank_reserves_inputs.csv\", sep=\" \", header=None)\n",
    "data_output = pd.read_csv(\"C:/Users/met48/Desktop/TS-Clustering/SimData/bank_reserves_outputs_poor.csv\", header=None)\n",
    "data = pd.concat([data_input, data_output], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the validation set to a new CSV file\n",
    "valid_data.to_csv(\"validation_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75220</th>\n",
       "      <td>0.848036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.109195</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.097701</td>\n",
       "      <td>0.091429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.092486</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48955</th>\n",
       "      <td>0.629694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398844</td>\n",
       "      <td>0.385057</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.403409</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>0.373563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44966</th>\n",
       "      <td>0.574623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.454023</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.485549</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.462857</td>\n",
       "      <td>0.448864</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.462428</td>\n",
       "      <td>0.459770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13568</th>\n",
       "      <td>0.244526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826590</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.867052</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.826590</td>\n",
       "      <td>0.804598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92727</th>\n",
       "      <td>0.769596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.205714</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.205714</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.179191</td>\n",
       "      <td>0.183908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3      4         5         6         7         8    \\\n",
       "75220  0.848036  0.0  0.0  0.0  0.125  0.083333  0.176471  0.318182  0.250000   \n",
       "48955  0.629694  0.0  0.0  0.0  0.000  0.083333  0.117647  0.318182  0.285714   \n",
       "44966  0.574623  0.0  0.0  0.0  0.000  0.083333  0.176471  0.363636  0.357143   \n",
       "13568  0.244526  0.0  0.0  0.0  0.125  0.083333  0.176471  0.363636  0.321429   \n",
       "92727  0.769596  0.0  0.0  0.0  0.250  0.166667  0.294118  0.318182  0.464286   \n",
       "\n",
       "            9    ...       92        93        94        95        96   \\\n",
       "75220  0.272727  ...  0.104046  0.109195  0.108571  0.104046  0.097701   \n",
       "48955  0.333333  ...  0.398844  0.385057  0.405714  0.393064  0.419540   \n",
       "44966  0.303030  ...  0.450867  0.454023  0.497143  0.485549  0.465517   \n",
       "13568  0.545455  ...  0.826590  0.839080  0.851429  0.867052  0.844828   \n",
       "92727  0.363636  ...  0.213873  0.212644  0.205714  0.202312  0.195402   \n",
       "\n",
       "            97        98        99        100       101  \n",
       "75220  0.091429  0.090909  0.093023  0.092486  0.091954  \n",
       "48955  0.405714  0.403409  0.406977  0.393064  0.373563  \n",
       "44966  0.462857  0.448864  0.453488  0.462428  0.459770  \n",
       "13568  0.828571  0.829545  0.837209  0.826590  0.804598  \n",
       "92727  0.205714  0.193182  0.186047  0.179191  0.183908  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame (assuming it's named `df`)\n",
    "dataset = TimeSeriesDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 1      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 101   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.00033997453283518553\n",
      "Epoch 2/40, Loss: 0.00011805783287854865\n",
      "Epoch 3/40, Loss: 0.0003187482652720064\n",
      "Epoch 4/40, Loss: 1.21088760351995e-05\n",
      "Epoch 5/40, Loss: 0.0001723524328554049\n",
      "Epoch 6/40, Loss: 2.203959957114421e-05\n",
      "Epoch 7/40, Loss: 3.7507875276787672e-06\n",
      "Epoch 8/40, Loss: 2.5105339318542974e-06\n",
      "Epoch 9/40, Loss: 0.0001484306703787297\n",
      "Epoch 10/40, Loss: 2.0865430997218937e-05\n",
      "Epoch 11/40, Loss: 4.1838011384243146e-06\n",
      "Epoch 12/40, Loss: 1.7407292034476995e-06\n",
      "Epoch 13/40, Loss: 1.4382562767423224e-05\n",
      "Epoch 14/40, Loss: 4.368371264718007e-06\n",
      "Epoch 15/40, Loss: 1.4602462670154637e-06\n",
      "Epoch 16/40, Loss: 1.5321724049499608e-06\n",
      "Epoch 17/40, Loss: 1.483291953263688e-06\n",
      "Epoch 18/40, Loss: 1.4466116908806725e-06\n",
      "Epoch 19/40, Loss: 4.437789584699203e-07\n",
      "Epoch 20/40, Loss: 1.9874409190379083e-05\n",
      "Epoch 21/40, Loss: 2.8447507247619797e-06\n",
      "Epoch 22/40, Loss: 1.6456745015602792e-06\n",
      "Epoch 23/40, Loss: 6.857779339952685e-07\n",
      "Epoch 24/40, Loss: 5.731689043386723e-07\n",
      "Epoch 25/40, Loss: 1.1031600024580257e-06\n",
      "Epoch 26/40, Loss: 1.1304595091132796e-06\n",
      "Epoch 27/40, Loss: 7.293248245332506e-07\n",
      "Epoch 28/40, Loss: 1.0313834764019703e-06\n",
      "Epoch 29/40, Loss: 4.319707329614175e-07\n",
      "Epoch 30/40, Loss: 2.1875104039281723e-07\n",
      "Epoch 31/40, Loss: 6.495390039162885e-07\n",
      "Epoch 32/40, Loss: 4.3303538177497103e-07\n",
      "Epoch 33/40, Loss: 2.082872185837914e-07\n",
      "Epoch 34/40, Loss: 4.800602368959517e-07\n",
      "Epoch 35/40, Loss: 2.2819492642156547e-06\n",
      "Epoch 36/40, Loss: 1.7487180912212352e-06\n",
      "Epoch 37/40, Loss: 5.576338253376889e-07\n",
      "Epoch 38/40, Loss: 5.134197635925375e-07\n",
      "Epoch 39/40, Loss: 1.1132373174405075e-06\n",
      "Epoch 40/40, Loss: 1.8844377791538136e-06\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40  # Adjust based on dataset size and performance\n",
    "train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTimeSeriesModel(\n",
       "  (encoder): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (expand_input): Linear(in_features=128, out_features=12928, bias=True)\n",
       "  (target_embedding): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bank_reserves_inputs_surr.csv', 'w', newline='') as bank_reserves_inputs, \\\n",
    "    open('bank_reserves_outputs_poor_surr.csv', 'w', newline='') as bank_reserves_outputs_poor:\n",
    "    for i in np.arange(100000):\n",
    "        reserve_perc = random.uniform(0,100)\n",
    "        bankRes = bankReservesModel.BankReserves(init_people=500, rich_threshold=10, reserve_percent=reserve_perc)\n",
    "        bankRes.run_model()\n",
    "        results = bankRes.datacollector.get_model_vars_dataframe()\n",
    "        print(reserve_perc, file=bank_reserves_inputs)\n",
    "        print(*results['Rich'].to_list(), file=bank_reserves_outputs_rich, sep=\",\")\n",
    "        print(*results['Middle Class'].to_list(), file=bank_reserves_outputs_middle, sep=\",\")\n",
    "        print(*results['Poor'].to_list(), file=bank_reserves_outputs_poor, sep=\",\")\n",
    "bank_reserves_inputs.close()\n",
    "bank_reserves_outputs_poor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m0.10\u001b[39m, \u001b[38;5;241m101\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 47\u001b[0m, in \u001b[0;36mTransformerTimeSeriesModel.forward\u001b[1;34m(self, x, target_seq)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, target_seq):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# x: Input features [Batch, 3]\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# target_seq: Target sequence for teacher forcing [Batch, 255, 1]\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Encode input features\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)  \u001b[38;5;66;03m# [Batch, d_model]\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Expand input to match sequence length\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     expanded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand_input(encoded_input)  \u001b[38;5;66;03m# [Batch, seq_length * d_model]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "model.forward(0.10, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
