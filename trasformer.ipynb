{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "siboVxRJ5pes"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Visualization tools\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe.iloc[:, :3].values, dtype=torch.float32)  # Input: [Batch, 3]\n",
    "        self.Y = torch.tensor(dataframe.iloc[:, 3:].values, dtype=torch.float32).unsqueeze(-1) # Output: [Batch, 255, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Time Steps\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    " \n",
    "        # Input Encoder (maps input to d_model size)\n",
    "        self.encoder = nn.Linear(input_dim, d_model)  # (Batch, 3) -> (Batch, d_model)\n",
    "        \n",
    "        # Project input to match the sequence length\n",
    "        self.expand_input = nn.Linear(d_model, seq_length * d_model)  # (Batch, d_model) -> (Batch, seq_length * d_model)\n",
    "        \n",
    "        # Target embedding for decoder input\n",
    "        self.target_embedding = nn.Linear(1, d_model)  # New embedding layer for target sequence\n",
    "  \n",
    "        # Positional Encoding for Time Steps\n",
    "        self.pos_encoder = PositionalEncoding(d_model, seq_length)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)  # (Batch, 255, d_model) -> (Batch, 255, 1)\n",
    "\n",
    "    def forward(self, x, target_seq):\n",
    "        # x: Input features [Batch, 3]\n",
    "        # target_seq: Target sequence for teacher forcing [Batch, 255, 1]\n",
    "        \n",
    "        # Encode input features\n",
    "        encoded_input = self.encoder(x)  # [Batch, d_model]\n",
    "        \n",
    "        # Expand input to match sequence length\n",
    "        expanded_input = self.expand_input(encoded_input)  # [Batch, seq_length * d_model]\n",
    "        expanded_input = expanded_input.view(-1, self.seq_length, self.d_model)  # Reshape to [Batch, 255, d_model]\n",
    "        \n",
    "        # Add Positional Encoding\n",
    "        expanded_input = self.pos_encoder(expanded_input)\n",
    "        \n",
    "        # Process the target sequence through the same encoding pipeline\n",
    "  #      target_embeddings = self.encoder(target_seq)\n",
    "  #      target_embeddings = nn.Linear(1, d_model)(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.target_embedding(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.pos_encoder(target_embeddings)\n",
    "        \n",
    "        # Decode sequence\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=target_embeddings, memory=expanded_input\n",
    "        )  # Output shape: [Batch, 255, d_model]\n",
    "        \n",
    "        # Map to output dimensions\n",
    "        predictions = self.output_layer(output)  # [Batch, 255, 1]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            x, y = batch  # x: [Batch, N], y: [Batch, T]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Prepare target for teacher forcing\n",
    "            target_seq = y \n",
    "            #target_seq = y[:, :-1]  # All except last time step\n",
    "            #actual = y[:, 1:]       # All except first time step\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(x, target_seq)\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data_input = pd.read_csv(\"C:/Users/met48/Desktop/TS-Clustering/SimData/epsteinCV_inputs.csv\", sep=\" \", header=None)\n",
    "data_output = pd.read_csv(\"C:/Users/met48/Desktop/TS-Clustering/SimData/epsteinCV_outputs_active.csv\", header=None)\n",
    "data = pd.concat([data_input, data_output], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the validation set to a new CSV file\n",
    "valid_data.to_csv(\"validation_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>0.553148</td>\n",
       "      <td>0.109513</td>\n",
       "      <td>0.033030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407305</td>\n",
       "      <td>0.478291</td>\n",
       "      <td>0.413508</td>\n",
       "      <td>0.341144</td>\n",
       "      <td>0.263267</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.007581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34861</th>\n",
       "      <td>0.965540</td>\n",
       "      <td>0.031016</td>\n",
       "      <td>0.216775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747760</td>\n",
       "      <td>0.853205</td>\n",
       "      <td>0.837354</td>\n",
       "      <td>0.824259</td>\n",
       "      <td>0.807030</td>\n",
       "      <td>0.795314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583046</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.591316</td>\n",
       "      <td>0.595451</td>\n",
       "      <td>0.594762</td>\n",
       "      <td>0.587870</td>\n",
       "      <td>0.593384</td>\n",
       "      <td>0.594073</td>\n",
       "      <td>0.589249</td>\n",
       "      <td>0.593384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18274</th>\n",
       "      <td>0.949296</td>\n",
       "      <td>0.038281</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604411</td>\n",
       "      <td>0.773260</td>\n",
       "      <td>0.760855</td>\n",
       "      <td>0.744314</td>\n",
       "      <td>0.724328</td>\n",
       "      <td>0.711923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493453</td>\n",
       "      <td>0.490696</td>\n",
       "      <td>0.494831</td>\n",
       "      <td>0.495520</td>\n",
       "      <td>0.492764</td>\n",
       "      <td>0.493453</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>0.504480</td>\n",
       "      <td>0.500345</td>\n",
       "      <td>0.492764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33070</th>\n",
       "      <td>0.720729</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239835</td>\n",
       "      <td>0.177808</td>\n",
       "      <td>0.038594</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29702</th>\n",
       "      <td>0.238838</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.037248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2    3         4         5         6    \\\n",
       "3547   0.553148  0.109513  0.033030  0.0  0.407305  0.478291  0.413508   \n",
       "34861  0.965540  0.031016  0.216775  0.0  0.747760  0.853205  0.837354   \n",
       "18274  0.949296  0.038281  0.495498  0.0  0.604411  0.773260  0.760855   \n",
       "33070  0.720729  0.248677  0.152559  0.0  0.239835  0.177808  0.038594   \n",
       "29702  0.238838  0.756370  0.037248  0.0  0.019297  0.000000  0.000000   \n",
       "\n",
       "            7         8         9    ...       245       246       247  \\\n",
       "3547   0.341144  0.263267  0.197795  ...  0.008959  0.002757  0.001378   \n",
       "34861  0.824259  0.807030  0.795314  ...  0.583046  0.579600  0.591316   \n",
       "18274  0.744314  0.724328  0.711923  ...  0.493453  0.490696  0.494831   \n",
       "33070  0.002068  0.000000  0.000000  ...  0.002068  0.000689  0.001378   \n",
       "29702  0.000000  0.000689  0.000000  ...  0.000689  0.000000  0.000000   \n",
       "\n",
       "            248       249       250       251       252       253       254  \n",
       "3547   0.002757  0.004824  0.006203  0.004135  0.006892  0.001378  0.007581  \n",
       "34861  0.595451  0.594762  0.587870  0.593384  0.594073  0.589249  0.593384  \n",
       "18274  0.495520  0.492764  0.493453  0.492074  0.504480  0.500345  0.492764  \n",
       "33070  0.002068  0.004135  0.000689  0.000689  0.002757  0.002068  0.001378  \n",
       "29702  0.000000  0.000000  0.001378  0.000000  0.000689  0.000689  0.000000  \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame (assuming it's named `df`)\n",
    "dataset = TimeSeriesDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.0004618491802830249\n",
      "Epoch 2/40, Loss: 0.03647257015109062\n",
      "Epoch 3/40, Loss: 0.0005646158242598176\n",
      "Epoch 4/40, Loss: 0.0011625391198322177\n",
      "Epoch 5/40, Loss: 0.00031830969965085387\n",
      "Epoch 6/40, Loss: 0.0046140761114656925\n",
      "Epoch 7/40, Loss: 0.000188781094038859\n",
      "Epoch 8/40, Loss: 0.3437782824039459\n",
      "Epoch 9/40, Loss: 0.0012508860090747476\n",
      "Epoch 10/40, Loss: 9.2369104095269e-05\n",
      "Epoch 11/40, Loss: 0.0004199365503154695\n",
      "Epoch 12/40, Loss: 4.388952947920188e-05\n",
      "Epoch 13/40, Loss: 0.00014055325300432742\n",
      "Epoch 14/40, Loss: 0.004239596892148256\n",
      "Epoch 15/40, Loss: 0.0024202109780162573\n",
      "Epoch 16/40, Loss: 3.474116601864807e-05\n",
      "Epoch 17/40, Loss: 1.0117051715496927e-05\n",
      "Epoch 18/40, Loss: 5.390956630435539e-06\n",
      "Epoch 19/40, Loss: 1.044252439896809e-05\n",
      "Epoch 20/40, Loss: 5.302300905896118e-06\n",
      "Epoch 21/40, Loss: 4.486436864681309e-06\n",
      "Epoch 22/40, Loss: 1.980277829716215e-06\n",
      "Epoch 23/40, Loss: 3.0563612654077588e-06\n",
      "Epoch 24/40, Loss: 4.802855983143672e-06\n",
      "Epoch 25/40, Loss: 8.978720870800316e-06\n",
      "Epoch 26/40, Loss: 5.0232802095706575e-06\n",
      "Epoch 27/40, Loss: 0.35354548692703247\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40  # Adjust based on dataset size and performance\n",
    "train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
