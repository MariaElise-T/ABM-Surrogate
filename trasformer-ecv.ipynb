{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "siboVxRJ5pes"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.X = torch.tensor(dataframe.iloc[:, :3].values, dtype=torch.float32)  # Input: [Batch, 3]\n",
    "        self.Y = torch.tensor(dataframe.iloc[:, 3:].values, dtype=torch.float32).unsqueeze(-1) # Output: [Batch, 255, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Time Steps\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    " \n",
    "        # Input Encoder (maps input to d_model size)\n",
    "        self.encoder = nn.Linear(input_dim, d_model)  # (Batch, 3) -> (Batch, d_model)\n",
    "        \n",
    "        # Project input to match the sequence length\n",
    "        self.expand_input = nn.Linear(d_model, seq_length * d_model)  # (Batch, d_model) -> (Batch, seq_length * d_model)\n",
    "        \n",
    "        # Target embedding for decoder input\n",
    "        self.target_embedding = nn.Linear(1, d_model)  # New embedding layer for target sequence\n",
    "  \n",
    "        # Positional Encoding for Time Steps\n",
    "        self.pos_encoder = PositionalEncoding(d_model, seq_length)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)  # (Batch, 255, d_model) -> (Batch, 255, 1)\n",
    "\n",
    "    def forward(self, x, target_seq):\n",
    "        # x: Input features [Batch, 3]\n",
    "        # target_seq: Target sequence for teacher forcing [Batch, 255, 1]\n",
    "        \n",
    "        # Encode input features\n",
    "        encoded_input = self.encoder(x)  # [Batch, d_model]\n",
    "        \n",
    "        # Expand input to match sequence length\n",
    "        expanded_input = self.expand_input(encoded_input)  # [Batch, seq_length * d_model]\n",
    "        expanded_input = expanded_input.view(-1, self.seq_length, self.d_model)  # Reshape to [Batch, 255, d_model]\n",
    "        \n",
    "        # Add Positional Encoding\n",
    "        expanded_input = self.pos_encoder(expanded_input)\n",
    "        \n",
    "        # Process the target sequence through the same encoding pipeline\n",
    "  #      target_embeddings = self.encoder(target_seq)\n",
    "  #      target_embeddings = nn.Linear(1, d_model)(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.target_embedding(target_seq)  # [Batch, 255, d_model]\n",
    "        target_embeddings = self.pos_encoder(target_embeddings)\n",
    "        \n",
    "        # Decode sequence\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=target_embeddings, memory=expanded_input\n",
    "        )  # Output shape: [Batch, 255, d_model]\n",
    "        \n",
    "        # Map to output dimensions\n",
    "        predictions = self.output_layer(output)  # [Batch, 255, 1]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs, device):\n",
    "    loss_list = list()\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            x, y = batch  # x: [Batch, N], y: [Batch, T]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Prepare target for teacher forcing\n",
    "            target_seq = y \n",
    "            #target_seq = y[:, :-1]  # All except last time step\n",
    "            #actual = y[:, 1:]       # All except first time step\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(x, target_seq)\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data_input = pd.read_csv(\"~/Desktop/TS-Clustering/SimData/epsteinCV_inputs.csv\", sep=\" \", header=None)\n",
    "data_output = pd.read_csv(\"~/Desktop/TS-Clustering/SimData/epsteinCV_outputs_active.csv\", header=None)\n",
    "data = pd.concat([data_input, data_output], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the validation set to a new CSV file\n",
    "valid_data.to_csv(\"validation_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>0.553148</td>\n",
       "      <td>0.109513</td>\n",
       "      <td>0.033030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407305</td>\n",
       "      <td>0.478291</td>\n",
       "      <td>0.413508</td>\n",
       "      <td>0.341144</td>\n",
       "      <td>0.263267</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.007581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34861</th>\n",
       "      <td>0.965540</td>\n",
       "      <td>0.031016</td>\n",
       "      <td>0.216775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747760</td>\n",
       "      <td>0.853205</td>\n",
       "      <td>0.837354</td>\n",
       "      <td>0.824259</td>\n",
       "      <td>0.807030</td>\n",
       "      <td>0.795314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583046</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.591316</td>\n",
       "      <td>0.595451</td>\n",
       "      <td>0.594762</td>\n",
       "      <td>0.587870</td>\n",
       "      <td>0.593384</td>\n",
       "      <td>0.594073</td>\n",
       "      <td>0.589249</td>\n",
       "      <td>0.593384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18274</th>\n",
       "      <td>0.949296</td>\n",
       "      <td>0.038281</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604411</td>\n",
       "      <td>0.773260</td>\n",
       "      <td>0.760855</td>\n",
       "      <td>0.744314</td>\n",
       "      <td>0.724328</td>\n",
       "      <td>0.711923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493453</td>\n",
       "      <td>0.490696</td>\n",
       "      <td>0.494831</td>\n",
       "      <td>0.495520</td>\n",
       "      <td>0.492764</td>\n",
       "      <td>0.493453</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>0.504480</td>\n",
       "      <td>0.500345</td>\n",
       "      <td>0.492764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33070</th>\n",
       "      <td>0.720729</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239835</td>\n",
       "      <td>0.177808</td>\n",
       "      <td>0.038594</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29702</th>\n",
       "      <td>0.238838</td>\n",
       "      <td>0.756370</td>\n",
       "      <td>0.037248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2    3         4         5         6    \\\n",
       "3547   0.553148  0.109513  0.033030  0.0  0.407305  0.478291  0.413508   \n",
       "34861  0.965540  0.031016  0.216775  0.0  0.747760  0.853205  0.837354   \n",
       "18274  0.949296  0.038281  0.495498  0.0  0.604411  0.773260  0.760855   \n",
       "33070  0.720729  0.248677  0.152559  0.0  0.239835  0.177808  0.038594   \n",
       "29702  0.238838  0.756370  0.037248  0.0  0.019297  0.000000  0.000000   \n",
       "\n",
       "            7         8         9    ...       245       246       247  \\\n",
       "3547   0.341144  0.263267  0.197795  ...  0.008959  0.002757  0.001378   \n",
       "34861  0.824259  0.807030  0.795314  ...  0.583046  0.579600  0.591316   \n",
       "18274  0.744314  0.724328  0.711923  ...  0.493453  0.490696  0.494831   \n",
       "33070  0.002068  0.000000  0.000000  ...  0.002068  0.000689  0.001378   \n",
       "29702  0.000000  0.000689  0.000000  ...  0.000689  0.000000  0.000000   \n",
       "\n",
       "            248       249       250       251       252       253       254  \n",
       "3547   0.002757  0.004824  0.006203  0.004135  0.006892  0.001378  0.007581  \n",
       "34861  0.595451  0.594762  0.587870  0.593384  0.594073  0.589249  0.593384  \n",
       "18274  0.495520  0.492764  0.493453  0.492074  0.504480  0.500345  0.492764  \n",
       "33070  0.002068  0.004135  0.000689  0.000689  0.002757  0.002068  0.001378  \n",
       "29702  0.000000  0.000000  0.001378  0.000000  0.000689  0.000689  0.000000  \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame (assuming it's named `df`)\n",
    "dataset = TimeSeriesDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 3      # Number of input features\n",
    "output_dim = 1     # Predicting one value per time step\n",
    "seq_length = 252   # Number of time steps in output\n",
    "d_model = 128      # Embedding dimension for Transformer\n",
    "nhead = 4          # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "dim_feedforward = 512  # Feedforward network size\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim, output_dim, seq_length, d_model, nhead, num_layers, dim_feedforward\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.00039567871135659516\n",
      "Epoch 2/40, Loss: 0.00016801682068035007\n",
      "Epoch 3/40, Loss: 8.223417535191402e-05\n",
      "Epoch 4/40, Loss: 0.00017714404384605587\n",
      "Epoch 5/40, Loss: 0.00023919713567011058\n",
      "Epoch 6/40, Loss: 3.275905328337103e-05\n",
      "Epoch 7/40, Loss: 0.00014134531375020742\n",
      "Epoch 8/40, Loss: 2.9211678338469937e-05\n",
      "Epoch 9/40, Loss: 3.313839988550171e-05\n",
      "Epoch 10/40, Loss: 1.0398732229077723e-05\n",
      "Epoch 11/40, Loss: 7.4640652201196644e-06\n",
      "Epoch 12/40, Loss: 7.13620056558284e-06\n",
      "Epoch 13/40, Loss: 6.548744295287179e-06\n",
      "Epoch 14/40, Loss: 1.201140275952639e-05\n",
      "Epoch 15/40, Loss: 0.022827692329883575\n",
      "Epoch 16/40, Loss: 0.00010908328840741888\n",
      "Epoch 17/40, Loss: 3.435953112784773e-05\n",
      "Epoch 18/40, Loss: 5.849183708050987e-06\n",
      "Epoch 19/40, Loss: 7.250102498801425e-06\n",
      "Epoch 20/40, Loss: 1.856780545494985e-05\n",
      "Epoch 21/40, Loss: 0.00037825110484845936\n",
      "Epoch 22/40, Loss: 0.00010496236791368574\n",
      "Epoch 23/40, Loss: 0.013372762128710747\n",
      "Epoch 24/40, Loss: 0.044779662042856216\n",
      "Epoch 25/40, Loss: 0.00012586344382725656\n",
      "Epoch 26/40, Loss: 0.1880200058221817\n",
      "Epoch 27/40, Loss: 3.141885463264771e-05\n",
      "Epoch 28/40, Loss: 1.4513105270452797e-05\n",
      "Epoch 29/40, Loss: 5.173385943635367e-05\n",
      "Epoch 30/40, Loss: 4.254893610777799e-06\n",
      "Epoch 31/40, Loss: 5.107910965307383e-06\n",
      "Epoch 32/40, Loss: 1.3102546745358268e-06\n",
      "Epoch 33/40, Loss: 1.452265109946893e-06\n",
      "Epoch 34/40, Loss: 6.211856089066714e-05\n",
      "Epoch 35/40, Loss: 0.014997536316514015\n",
      "Epoch 36/40, Loss: 7.544259278802201e-05\n",
      "Epoch 37/40, Loss: 0.0002103762817569077\n",
      "Epoch 38/40, Loss: 7.225142326205969e-06\n",
      "Epoch 39/40, Loss: 2.344506583540351e-06\n",
      "Epoch 40/40, Loss: 0.0027967856731265783\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()  # Regression loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40  # Adjust based on dataset size and performance\n",
    "train_model(model, dataloader, optimizer, loss_fn, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_adam_lr001_epstein.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/met48/Desktop/ABM-Surrogate/transformer_adam_lr001_epstein.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerTimeSeriesModel(\n\u001b[0;32m      2\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m252\u001b[39m, \n\u001b[0;32m      3\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim_feedforward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/met48/Desktop/ABM-Surrogate/transformer_adam_lr001_epstein.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/met48/Desktop/ABM-Surrogate/transformer_adam_lr001_epstein.pth'"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "    input_dim=3, output_dim=1, seq_length=252, \n",
    "    d_model=128, nhead=4, num_layers=2, dim_feedforward=512\n",
    ")\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(\"C:/Users/met48/Desktop/ABM-Surrogate/transformer_adam_lr001_epstein.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"transformer_adam_lr001_epstein.pth\", map_location=torch.device(\"cpu\")))\n",
    "with open('epsteinCV_inputs_surrogate.csv', 'w', newline='') as epsteinCV_inputs, \\\n",
    "    open('epsteinCV_outputs_active_surrogate.csv', 'w', newline='') as epsteinCV_outputs_active:\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    batch_size = 1  # Inference on a single sample\n",
    "    seq_length = 252\n",
    "    for i in np.arange(10000):\n",
    "        cit_dens = random.uniform(0.5, 1)\n",
    "        cop_dens = random.uniform(0, 1-cit_dens)\n",
    "        leg = random.uniform(0, 1)\n",
    "        input_features = torch.tensor([[cit_dens, cop_dens, leg]], dtype=torch.float32)  # Shape: [1, 3]\n",
    "        \n",
    "        # Initialize target sequence with zeros for inference\n",
    "        target_seq = torch.zeros((batch_size, seq_length, 1), dtype=torch.float32)  # Shape: [1, 252, 1]\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_features, target_seq)  # Output shape: [1, 252, 1]\n",
    "        \n",
    "        # Convert to NumPy array for easy viewing\n",
    "        predicted_values = predictions.squeeze().numpy()  # Shape: [255]\n",
    "        print(*list([cit_dens, cop_dens, leg]), file=epsteinCV_inputs)\n",
    "        epsteinCV_outputs_active.write(\" \".join(f\"{value:.6f}\" for value in predicted_values.flatten()))\n",
    "        #print(predicted_values.flatten().tolist(), file=epsteinCV_outputs_active, sep=\",\")\n",
    "epsteinCV_inputs.close()\n",
    "epsteinCV_outputs_active.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "train_inputs = pd.read_csv('epsteinCV_outputs_active_surrogate.csv', sep=' ')\n",
    "X_train = to_time_series_dataset(train_inputs)\n",
    "X_train_flat = [xi.flatten() for xi in X_train]\n",
    "\n",
    "km = TimeSeriesKMeans(verbose=True, random_state=seed)\n",
    "visualizer = KElbowVisualizer(km, k=(2,16), locate_elbow=False)\n",
    " \n",
    "visualizer.fit(np.array(X_train_flat))        \n",
    "visualizer.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_cluster_visualization(y_pred, df, n_clusters, plot_title):\n",
    "    ts_size = df.shape[1]\n",
    "    ts_max = df.max()\n",
    "    plt.figure()\n",
    "    for cluster in range(n_clusters):\n",
    "        plt.subplot(4, math.ceil(n_clusters/4), cluster+1)\n",
    "        for ts in df[y_pred == cluster]:\n",
    "            plt.plot(ts.ravel(), \"k-\", alpha=.2)\n",
    "        plt.plot(np.mean(df[y_pred == cluster], axis=0), \"r-\")\n",
    "        plt.xlim(0, ts_size)\n",
    "        plt.ylim(0, ts_max)\n",
    "        plt.text(0.55, 0.35,'Cluster %d' % (cluster),\n",
    "                 transform=plt.gca().transAxes)\n",
    "        if cluster == 1:\n",
    "            plt.title(plot_title)      \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
